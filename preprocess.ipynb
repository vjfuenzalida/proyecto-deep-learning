{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd8a45658b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type))\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as fh:\n",
    "        source = json.load(fh)\n",
    "        for article in tqdm(source[\"data\"]):\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"].replace(\n",
    "                    \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                context_tokens = word_tokenize(context)\n",
    "                context_chars = [list(token) for token in context_tokens]\n",
    "                spans = convert_idx(context, context_tokens)\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"])\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"])\n",
    "                for qa in para[\"qas\"]:\n",
    "                    total += 1\n",
    "                    ques = qa[\"question\"].replace(\n",
    "                        \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                    ques_tokens = word_tokenize(ques)\n",
    "                    ques_chars = [list(token) for token in ques_tokens]\n",
    "                    for token in ques_tokens:\n",
    "                        word_counter[token] += 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1\n",
    "                    y1s, y2s = [], []\n",
    "                    answer_texts = []\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = []\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                                answer_span.append(idx)\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1)\n",
    "                        y2s.append(y2)\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example)\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]}\n",
    "        random.shuffle(examples)\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    if emb_file is not None:\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.split()\n",
    "                word = \"\".join(array[0:-vec_size])\n",
    "                vector = list(map(float, array[-vec_size:]))\n",
    "                if word in counter and counter[word] > limit:\n",
    "                    embedding_dict[word] = vector\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.1) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 1)}\n",
    "    idx2token_dict={}\n",
    "    idx2token_dict[0]=NULL\n",
    "    idx2token_dict[len(embedding_dict)+1]=OOV\n",
    "    for k in token2idx_dict:\n",
    "        idx2token_dict[token2idx_dict[k]]=k\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = len(embedding_dict)+1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = np.random.random((1,vec_size))/2-0.25\n",
    "    idx2emb_dict = {idx: embedding_dict[token] for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict, idx2token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [01:16<00:00,  5.14it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77811 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  4.81it/s]\n",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9788 questions in total\n",
      "Generating dev examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570 questions in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "train_examples, train_eval = process_file('../../fwei/data/squad/train-v1.2.json', \"train\", word_counter, char_counter)\n",
    "dev_examples, dev_eval = process_file('../../fwei/data/squad/dev-v1.2.json', \"dev\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('../../fwei/data/squad/dev-v1.1.json', \"dev\", word_counter, char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval\n",
    "with open('dataset2/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "with open('dataset2/dev_eval.json','w') as fh:\n",
    "    json.dump(dev_eval,fh)\n",
    "with open('dataset2/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 782/2200000 [00:00<04:41, 7815.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2196017/2200000 [03:17<00:00, 11146.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91587 / 111136 tokens have corresponding word embedding vector\n",
      "Generating char embedding...\n",
      "1425 tokens have corresponding embedding vector\n"
     ]
    }
   ],
   "source": [
    "word_emb_mat, word2idx_dict, id2word_dict = get_embedding(\n",
    "    word_counter, \"word\", emb_file='../../fwei/data/glove/glove.840B.300d.txt', size=int(2.2e6), vec_size=300)\n",
    "char_emb_mat, char2idx_dict, id2char_dict = get_embedding(\n",
    "        char_counter, \"char\", emb_file=None, size=None, vec_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save id2word\n",
    "import pandas as pd\n",
    "df_id2word=[]\n",
    "for k in id2word_dict:\n",
    "    df_id2word.append([k,id2word_dict[k]])\n",
    "df_id2word=pd.DataFrame(df_id2word)\n",
    "df_id2word.to_csv('id2word.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91589\n",
      "1426\n",
      "(91589, 300)\n"
     ]
    }
   ],
   "source": [
    "word_size=len(word_emb_mat)\n",
    "char_input_size=len(char_emb_mat)-1\n",
    "print(word_size)\n",
    "print(char_input_size)\n",
    "word_mat=np.zeros((len(word_emb_mat),len(word_emb_mat[0])))\n",
    "for i,w in enumerate(word_emb_mat):\n",
    "    word_mat[i,:]=w\n",
    "print(word_mat.shape)\n",
    "np.save('word_emb_mat2.npy',word_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(char2idx_dict)\n",
    "# sorted(char_counter.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "# print(char_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77811/77811 [00:29<00:00, 2628.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 88534\n",
      "miss char: 0\n",
      "over limit: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9788/9788 [00:03<00:00, 2669.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 11241\n",
      "miss char: 0\n",
      "over limit: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:04<00:00, 2327.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss word: 13780\n",
      "miss char: 0\n",
      "over limit: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "def get_indexs(exa, word2idx_dict, char2idx_dict, cont_limit=400, ques_limit=50, ans_limit=30, char_limit=16):\n",
    "    n=len(exa)\n",
    "    miss_word=0\n",
    "    miss_char=0\n",
    "    cont_index=np.zeros((n,cont_limit))\n",
    "    ques_index=np.zeros((n,ques_limit))\n",
    "    cont_char_index=np.zeros((n,cont_limit,char_limit))\n",
    "    ques_char_index=np.zeros((n,ques_limit,char_limit))\n",
    "    cont_len=np.zeros((n,1))\n",
    "    ques_len=np.zeros((n,1))\n",
    "    y_start = np.zeros((n,cont_limit))\n",
    "    y_end = np.zeros((n,cont_limit))\n",
    "    qid = np.zeros((n))\n",
    "    overlimit=0\n",
    "    \n",
    "    # cont\n",
    "    for i in tqdm(range(n)):\n",
    "        qid[i]=int(exa[i]['id'])\n",
    "        \n",
    "        contexts=exa[i]['context_tokens']\n",
    "        cont_len[i,0]=min(cont_limit,len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j>=cont_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                cont_index[i,j]=word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word+=1\n",
    "                cont_index[i,j]=word2idx_dict['--OOV--']\n",
    "        contexts_char=exa[i]['context_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j>=cont_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2>=char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    cont_char_index[i,j,j2]=char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char+=1\n",
    "                    cont_char_index[i,j,j2]=char2idx_dict['--OOV--']\n",
    "        # ans\n",
    "        st=exa[i]['y1s'][0]\n",
    "        ed=exa[i]['y2s'][0]\n",
    "        if st<cont_limit:\n",
    "            y_start[i,st]=1\n",
    "        if ed<cont_limit:\n",
    "            if ed-st>ans_limit:\n",
    "                y_end[i,st+ans_limit]=1\n",
    "                overlimit+=1\n",
    "            else:\n",
    "                y_end[i,ed]=1\n",
    "        \n",
    "        # ques\n",
    "        contexts=exa[i]['ques_tokens']\n",
    "        ques_len[i,0]=min(ques_limit,len(contexts))\n",
    "        for j,c in enumerate(contexts):\n",
    "            if j>=ques_limit:\n",
    "                break\n",
    "            if c in word2idx_dict:\n",
    "                ques_index[i,j]=word2idx_dict[c]\n",
    "            else:\n",
    "                miss_word+=1\n",
    "                ques_index[i,j]=word2idx_dict['--OOV--']\n",
    "        contexts_char=exa[i]['ques_chars']\n",
    "        for j,c in enumerate(contexts_char):\n",
    "            if j>=ques_limit:\n",
    "                break\n",
    "            for j2,c2 in enumerate(c):\n",
    "                if j2>=char_limit:\n",
    "                    break\n",
    "                if c2 in char2idx_dict:\n",
    "                    ques_char_index[i,j,j2]=char2idx_dict[c2]\n",
    "                else:\n",
    "                    miss_char+=1\n",
    "                    ques_char_index[i,j,j2]=char2idx_dict['--OOV--']\n",
    "    print('miss word:',miss_word)\n",
    "    print('miss char:',miss_char)\n",
    "    print('over limit:',overlimit)\n",
    "        \n",
    "    return cont_index, ques_index, cont_char_index, ques_char_index, cont_len, ques_len, y_start, y_end, qid\n",
    "\n",
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(train_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset2/train_contw_input.npy',contw_input)\n",
    "np.save('dataset2/train_quesw_input.npy',quesw_input)\n",
    "np.save('dataset2/train_contc_input.npy',contc_input)\n",
    "np.save('dataset2/train_quesc_input.npy',quesc_input)\n",
    "np.save('dataset2/train_cont_len.npy',cont_len)\n",
    "np.save('dataset2/train_ques_len.npy',ques_len)\n",
    "np.save('dataset2/train_y_start.npy',y_start)\n",
    "np.save('dataset2/train_y_end.npy',y_end)\n",
    "np.save('dataset2/train_qid.npy',qid)\n",
    "\n",
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(dev_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset2/dev_contw_input.npy',contw_input)\n",
    "np.save('dataset2/dev_quesw_input.npy',quesw_input)\n",
    "np.save('dataset2/dev_contc_input.npy',contc_input)\n",
    "np.save('dataset2/dev_quesc_input.npy',quesc_input)\n",
    "np.save('dataset2/dev_cont_len.npy',cont_len)\n",
    "np.save('dataset2/dev_ques_len.npy',ques_len)\n",
    "np.save('dataset2/dev_y_start.npy',y_start)\n",
    "np.save('dataset2/dev_y_end.npy',y_end)\n",
    "np.save('dataset2/dev_qid.npy',qid)\n",
    "\n",
    "contw_input, quesw_input, contc_input, quesc_input, cont_len, ques_len, y_start, y_end, qid\\\n",
    "=get_indexs(test_examples, word2idx_dict, char2idx_dict)\n",
    "\n",
    "np.save('dataset2/test_contw_input.npy',contw_input)\n",
    "np.save('dataset2/test_quesw_input.npy',quesw_input)\n",
    "np.save('dataset2/test_contc_input.npy',contc_input)\n",
    "np.save('dataset2/test_quesc_input.npy',quesc_input)\n",
    "np.save('dataset2/test_cont_len.npy',cont_len)\n",
    "np.save('dataset2/test_ques_len.npy',ques_len)\n",
    "np.save('dataset2/test_y_start.npy',y_start)\n",
    "np.save('dataset2/test_y_end.npy',y_end)\n",
    "np.save('dataset2/test_qid.npy',qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77811, 400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.load('dataset2/train_y_start.npy') \n",
    "print(a.shape)\n",
    "# print(np.argmax(a,axis=1))\n",
    "# a=np.load('dataset2/dev_y_end.npy') \n",
    "# print(np.argmax(a,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
